{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b580e50-e233-4d75-884d-f9ac1abdaedc",
   "metadata": {},
   "source": [
    "#### Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3557c77e-34c4-4915-9e1e-d8c756616f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gerzem1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python basics\n",
    "from pprint import pprint\n",
    "import re\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import json\n",
    "import os\n",
    "\n",
    "# NLP & Embeddings\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import nltk       \n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be651fb-8d08-46a4-bef5-8dc5c903aaa4",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cade024-efd1-46c5-8fb4-6e42d0afb452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import (\n",
    "    CLEANED_DATA_DIR, \n",
    "    SITES_ELEMENTS_JSON_PATH,\n",
    "    URL_PATTERN, \n",
    "    REPLACED_URL_MAP_PATH, \n",
    "    SITES_ELEMENTS_WITH_REPLACED_URLS_PATH,\n",
    "    MAX_TOKENS, \n",
    "    OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06db7a-f0a9-4d69-b2f7-dfa8f16446cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Tokenizer & Embedding Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "004ca9ec-78ad-4257-a9bd-7184d99e0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0fad1-fd4a-45b7-95b9-dc7ea912689a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Handle urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aafb0e48-a086-487a-87d9-dea2d005e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_url_saved(saved_urls: Dict[str, str], url: str):\n",
    "    for saved_url_label, saved_url in saved_urls.items():\n",
    "        if saved_url == url:\n",
    "            return saved_url_label\n",
    "    return False\n",
    "\n",
    "def extract_and_replace_urls(\n",
    "    text: str, last_url_number: int, \n",
    "    replaced_url_map: Dict[str, str], \n",
    "    url_pattern: str = URL_PATTERN\n",
    ") -> (str, int):\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    for url in urls:\n",
    "        saved_url_label = is_url_saved(replaced_url_map, url)\n",
    "        if isinstance(saved_url_label, str):\n",
    "            placeholder = saved_url_label\n",
    "        else:\n",
    "            placeholder = f\"[URL_{last_url_number}]\"\n",
    "            last_url_number += 1\n",
    "            replaced_url_map[placeholder] = url\n",
    "        \n",
    "        text = text.replace(f\"[{url}]\", placeholder)\n",
    "    return text, last_url_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cfdd4f56-a428-4c7f-8748-405f53ae2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_elements_with_replaced_urls = []\n",
    "last_url_number = 0\n",
    "\n",
    "if os.path.exists(REPLACED_URL_MAP_PATH):\n",
    "    with open(REPLACED_URL_MAP_PATH, 'r', encoding='utf-8') as file_m:\n",
    "        replaced_url_map = json.load(file_m)\n",
    "else:\n",
    "    replaced_url_map = {}\n",
    "\n",
    "if os.path.exists(SITES_ELEMENTS_JSON_PATH):\n",
    "    with open(SITES_ELEMENTS_JSON_PATH, 'r', encoding='utf-8') as file_s:\n",
    "        sites_elements = json.load(file_s)\n",
    "else:\n",
    "    sites_elements = []\n",
    "\n",
    "for site_element in sites_elements:\n",
    "    site_element_with_replaced_urls, last_url_number = extract_and_replace_urls(site_element['text'], last_url_number, replaced_url_map, URL_PATTERN)\n",
    "    site_element_with_replaced_urls = {\n",
    "        'text': site_element_with_replaced_urls,\n",
    "        'source': site_element['source']\n",
    "    }\n",
    "    sites_elements_with_replaced_urls.append(site_element_with_replaced_urls)\n",
    "    \n",
    "with open(SITES_ELEMENTS_WITH_REPLACED_URLS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sites_elements_with_replaced_urls, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(REPLACED_URL_MAP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(replaced_url_map, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5297f-4764-4f2f-9e0e-3c75745a6c1e",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "78266d3e-15fe-4fef-bf9e-a4f4159c4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str, max_tokens: Optional[int] = 400, overlap: Optional[int] = 50):\n",
    "    \"\"\"\n",
    "    Splits `text` into token-based chunks with overlap and tokenizes it.\n",
    "    Returns a list of plain‑text chunks and tokens\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        # tells the tokenizer to return the start and end character positions\n",
    "        # of each token relative to the original text.\n",
    "        return_offsets_mapping=True,\n",
    "\n",
    "        # disables automatic insertion of tokens like [CLS], [SEP], etc., \n",
    "        # which can interfere with chunking.\n",
    "        add_special_tokens=False,\n",
    "        truncation=True  # suppresses warning\n",
    "    )\n",
    "\n",
    "    # a list of integers representing the tokens.\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    if not input_ids:\n",
    "        return [], []\n",
    "\n",
    "    \n",
    "    tokens_list = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # offsets is a list of tuples: each tuple is (start_char_index, end_char_index)\n",
    "    # — where each token lives in the original text.\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "    \n",
    "    chunks = []\n",
    "    tokens = []\n",
    "    start_token = 0\n",
    "    while start_token < len(input_ids):\n",
    "        end_token = min(start_token + max_tokens, len(input_ids))\n",
    "        chunk_offsets = offsets[start_token:end_token]\n",
    "\n",
    "        chunk_start_char = chunk_offsets[0][0]\n",
    "        chunk_end_char = chunk_offsets[-1][1]\n",
    "\n",
    "        chunk_text = text[chunk_start_char:chunk_end_char]\n",
    "        chunks.append(chunk_text)\n",
    "        # chunks.append({\n",
    "        #     \"text\": chunk_text.strip(),\n",
    "        #     \"start_char\": chunk_start_char,\n",
    "        #     \"end_char\": chunk_end_char,\n",
    "        #     \"token_count\": end_token - start_token\n",
    "        # })\n",
    "        tokens.append(tokens_list[start_token:end_token])\n",
    "        start_token += max_tokens - overlap\n",
    "    \n",
    "    return chunks, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e4423393-556a-4c89-9222-5ac3a519e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites_elements = [\n",
    "#     {\n",
    "#         'text': 'This is some text with a link [URL_0] and more explanation. Another [URL_1] appears.',\n",
    "#         'source': \"\"\n",
    "#     },\n",
    "#     {\n",
    "#         'text': 'This is some text with a link [URL_0] and more explanation. Another [URL_1] appears.',\n",
    "#         'source': \"\"\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "63eb3fd6-4389-4ad4-b50e-9f7ef5e03eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(SITES_ELEMENTS_WITH_REPLACED_URLS_PATH):\n",
    "    with open(SITES_ELEMENTS_WITH_REPLACED_URLS_PATH, 'r', encoding='utf-8') as file_s:\n",
    "        sites_elements = json.load(file_s)\n",
    "else:\n",
    "    raise ValueError(f\"Failed to open {SITES_ELEMENTS_WITH_REPLACED_URLS_PATH}\")\n",
    "\n",
    "chunk_list = []\n",
    "token_list = []\n",
    "for site_element in sites_elements:\n",
    "    chunks, tokens = tokenize(site_element['text'], max_tokens=MAX_TOKENS, overlap=OVERLAP)\n",
    "    # for 1 chunk, token_list has 1 array of tokens\n",
    "    chunk_list.extend(chunks)\n",
    "    token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422b43e-61b2-4f6e-90ad-12e5e044aa02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
