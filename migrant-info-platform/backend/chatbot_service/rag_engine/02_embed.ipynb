{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b580e50-e233-4d75-884d-f9ac1abdaedc",
   "metadata": {},
   "source": [
    "#### Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3557c77e-34c4-4915-9e1e-d8c756616f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gerzem1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python basics\n",
    "from pprint import pprint\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "import json\n",
    "import os\n",
    "\n",
    "# NLP & Embeddings\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import nltk       \n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be651fb-8d08-46a4-bef5-8dc5c903aaa4",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cade024-efd1-46c5-8fb4-6e42d0afb452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import (\n",
    "    CLEANED_DATA_DIR, \n",
    "    SITES_ELEMENTS_JSON_PATH,\n",
    "    URL_PATTERN, \n",
    "    REPLACED_URL_MAP_PATH, \n",
    "    SITES_ELEMENTS_WITH_REPLACED_URLS_PATH,\n",
    "    MAX_TOKENS, \n",
    "    OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06db7a-f0a9-4d69-b2f7-dfa8f16446cc",
   "metadata": {},
   "source": [
    "#### Tokenizer & Embedding Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "004ca9ec-78ad-4257-a9bd-7184d99e0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0fad1-fd4a-45b7-95b9-dc7ea912689a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Handle urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aafb0e48-a086-487a-87d9-dea2d005e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_url_saved(saved_urls: Dict[str, str], url: str):\n",
    "    for saved_url_label, saved_url in saved_urls.items():\n",
    "        if saved_url == url:\n",
    "            return saved_url_label\n",
    "    return False\n",
    "\n",
    "def extract_and_replace_urls(\n",
    "    text: str, last_url_number: int, \n",
    "    replaced_url_map: Dict[str, str], \n",
    "    url_pattern: str = URL_PATTERN\n",
    ") -> (str, int):\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    for url in urls:\n",
    "        saved_url_label = is_url_saved(replaced_url_map, url)\n",
    "        if isinstance(saved_url_label, str):\n",
    "            placeholder = saved_url_label\n",
    "        else:\n",
    "            placeholder = f\"[URL_{last_url_number}]\"\n",
    "            last_url_number += 1\n",
    "            replaced_url_map[placeholder] = url\n",
    "        \n",
    "        text = text.replace(f\"[{url}]\", placeholder)\n",
    "    return text, last_url_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfdd4f56-a428-4c7f-8748-405f53ae2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_elements_with_replaced_urls = []\n",
    "replaced_url_map = {}\n",
    "last_url_number = 0\n",
    "\n",
    "if os.path.exists(SITES_ELEMENTS_JSON_PATH):\n",
    "    with open(SITES_ELEMENTS_JSON_PATH, 'r', encoding='utf-8') as file_s:\n",
    "        sites_elements = json.load(file_s)\n",
    "else:\n",
    "    sites_elements = []\n",
    "\n",
    "for site_element in sites_elements:\n",
    "    site_element_with_replaced_urls, last_url_number = extract_and_replace_urls(site_element['text'], last_url_number, replaced_url_map, URL_PATTERN)\n",
    "    sites_elements_with_replaced_urls.append(site_element_with_replaced_urls)\n",
    "    \n",
    "with open(SITES_ELEMENTS_WITH_REPLACED_URLS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sites_elements_with_replaced_urls, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(REPLACED_URL_MAP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(replaced_url_map, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5297f-4764-4f2f-9e0e-3c75745a6c1e",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78266d3e-15fe-4fef-bf9e-a4f4159c4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text: str, max_tokens: Optional[int] = 400, overlap: Optional[int] = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits `text` into token-based chunks with overlap.\n",
    "    Returns a list of plainâ€‘text chunks.\n",
    "    \"\"\"\n",
    "    max_tokens = max_tokens if max_tokens is not None else self.max_tokens\n",
    "    overlap = overlap if overlap is not None else self.overlap\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "    \n",
    "    chunks = []\n",
    "    start_token = 0\n",
    "    while start_token < len(input_ids):\n",
    "        end_token = min(start_token + max_tokens, len(input_ids))\n",
    "        chunk_offsets = offsets[start_token:end_token]\n",
    "\n",
    "        chunk_start_char = chunk_offsets[0][0]\n",
    "        chunk_end_char = chunk_offsets[-1][1]\n",
    "\n",
    "        chunk_text = text[chunk_start_char:chunk_end_char]\n",
    "        chunks.append({\n",
    "            \"text\": chunk_text.strip(),\n",
    "            \"start_char\": chunk_start_char,\n",
    "            \"end_char\": chunk_end_char,\n",
    "            \"token_count\": end_token - start_token\n",
    "        })\n",
    "\n",
    "        start_token += max_tokens - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63eb3fd6-4389-4ad4-b50e-9f7ef5e03eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'text': 'This is some text with a link [URL_0] and more explanation. Another [URL_1] appears.', 'start_char': 0, 'end_char': 84, 'token_count': 26}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sites_elements = [\n",
    "    {\n",
    "        'text': 'This is some text with a link [URL_0] and more explanation. Another [URL_1] appears.',\n",
    "        'source': \"\"\n",
    "    },\n",
    "    {\n",
    "        'text': 'This is some text with a link [URL_0] and more explanation. Another [URL_1] appears.',\n",
    "        'source': \"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "chunks = split_into_chunks(sites_elements[0]['text'], max_tokens=MAX_TOKENS, overlap=OVERLAP)\n",
    "pprint(len(chunks))\n",
    "for c in chunks:\n",
    "    print(f\"{c}\\n\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e849a-90fe-4e4d-98ba-6b8060411202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
